\section{Inference}

In this section we describe how the model is fitted in a Bayesian context. Section \ref{mcmc_section} describes the McMC algorithm used to fit this model, the next section (\ref{priors_section}) develops a prior model.

\subsection{McMC algorithm} \label{mcmc_section}

The joint model over all unobserved (the parameters) and observed (the data) quantities is fitted using standard Markov chain Monte Carlo (McMC) algorithms (full details in Algorithm \ref{alg:mcmc_full} in the Appendix). The source effects and the 
relative prevalence matrix parameters 
are updated using independent adaptive Metropolis-Hastings updates \citep{RobRos06}. 
%After a given item in the parameter vector is updated, the remaining values are rescaled to sum to 1 (as required when using a Dirichlet prior). 

For the Dirichlet process assumed for the type effects $\bm{q}$ (Equation \ref{eq:qDP}) we choose $P_0 \sim \mbox{Gamma}(\alpha_\theta, \beta_\theta)$.  The choice of this distribution is conjugate 
with respect to the Poisson likelihood (Equation \ref{eq:likelihood}) allowing us to use a marginal Gibbs sampler within a Polya Urn, or  ``Chinese restaurant process'', construction \citep{GelCarSte13}.  Each type effect $q_i$ is associated with a group assignment variable $S_i$, such that $S_i = k$ assigns $q_i$ to group $k$.  Furthermore, each group $k$ assumes a value $\theta_k$ such that $q_i = \theta_{S_i}$.  The algorithm alternates between updating group assignments $\bm{S}$ and group values $\bm{\theta}$ as shown in Algorithm~ \ref{alg:mcmc}.  Hence, the algorithm explores the number of groups present, the type effects assigned to each group, and the values of each group.

\begin{algorithm}
\begin{algorithmic} 
%\bigskip{}
%\State \textbf{Initialise}$\colon$ Setup output matrices and initial values
\bigskip{}

%\For {$z$ in $1 \colon niter$}
%\bigskip{}
%\State \textbf{Step 1} $\colon$ Update source effects ($\mathbf{a}$) for each time $t$ and location $l$ (adaptive single site Normal random walk Metropolis-Hastings). 

%\bigskip{}
%\State \textbf{Step 2} $\colon$ Update components of the relative prevalence matrices ($r_{ij}$) for each time $t$ (adaptive single site Normal random walk Metropolis-Hastings). %% Details of this update?

%\bigskip{}
%\State Update type effects ($\mathbf{q}$) using a blocked Gibbs sampler (chinese restaurant construction).
%\bigskip{}
\ForEach {iteration z} 
\State \textbf{Update group allocations} ($S_{i}$)
\For {$i$ in $1 \colon I$}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent\relax}{%
    \setlength{\hangindent}{\algorithmicindent}%
      Assign type $i$ to group an existing group $k$ with probability $P(Si=k)$ or a new group $K+1$ with probability $P(Si=K+1)$ where}\strut
\State $\>\>\>\>\>\> P(Si=k)\propto n_{-k} L\left(y_{i}|\cdot\right)= \frac{n_{-k}}{y_i!} \theta_{k}^{y_{i}} e^{-\left(\theta_{k}\sum_{j=1}^{m}
a_{j}p_{ij}\right)}$ and
%\State $\>\>\>\>\>\>\>$ and 
\State $\>\>\>\>\>\> P\left(Si=K+1\right)\propto\alpha_q\int L\left(y_{i}|\cdot\right) Q_{0}\left(\theta\right)d\theta=\frac{\alpha_q}{y_i!}\frac{\beta_{\theta}^{\alpha_{\theta}}}{\Gamma
\left(\alpha_{\theta}\right)}\frac{\Gamma\left(\alpha_{\theta}+y_{i}\right)}{\left(\beta_{\theta}+\sum_{j=1}^{m}a_{j}p_{ij}\right)^{\alpha_{\theta}+y_{i}}}$
%\State where $L(y_{i}|\cdot)$ is the likelihood of the data given all the parameters evaluated at 
%\State $q_{i}=\mathbf{\theta_{c}}$ for all types $i$ in group $c$ at iteration $z$.
\If{$Si=K+1$}
\State Draw a $\theta$ value for the new group
\State $\theta_{K+1} \sim \textsf{Gamma}\left(a_{q}+y_{i}, b_{q}+\sum_{j=1}^{m}a_{j} p_{ij}\right)$
\EndIf
\EndFor
\medskip{}
\State \textbf{Update all group values} ($\theta_{k}$)
\State $\>\>\>\>\>\> \theta_{k} \sim \textsf{Gamma}\left(a_{q}+n_{k} y_{i}, b_{q}+n_{k} \sum_{j=1}^{m}a_{j} p_{ij}\right)$
%\State where $a_{q}^{*}=a_{q}+\sum_{i:S_{i}=k}y_{i}, b_{q}^{*}=b_{q}+\sum_{i:S_{i}=k}\sum_{j=1}^{J}a_{j}\cdot p_{ij}$
\State $\>\>\>\>\>\> q_{i}=\theta_{k}$ for all types $i$ in group $k$ at iteration $z$.
\EndFor
\State Note: $n_{k}$ is the number of types currently assigned to group $k$, and hence $n_{-k}$ is the number of types not currently assigned to group $k$.
\bigskip{}
\end{algorithmic}
\caption{Chinese restaurant algorithm to update the type effects.}
\label{alg:mcmc}
\end{algorithm}

The computation time and memory consumption can be significantly reduced using 
the settings in the \code{mcmc\_params} argument. Special attention must be paid to \code{n\_r} which 
controls the number of elements in the relative prevalence matrix (\code{n\_r}) that get updated at each 
iteration. Reducing this number can significantly reduce run time, however, the chains will converge more 
slowly. Other options include not saving the burn in or thinned iterations, or the $\lambda_i$ and $\lambda_j$ values at each iteration (as they can be computed ad hoc from the other parameters).

\subsection{Priors} \label{priors_section}

As the source and type parameters have no simple biological significance (they account for a multitude of source and type specific factors), there is no obvious prior with known distribution form or 
value that can be defined for these parameters. Hence, prior distributions were chosen for computational convenience (prior parameters can be chosen so that the priors are relatively non-
informative). A Dirichlet prior is placed on the relative prevalences for each source $j$ ($\mathbf{r}_j$) which correctly constrains $\sum_{i=1} ^n r_{ij}$ to equal one. A Dirichlet prior is also placed on 
the source effects which constrains their scale, aiding identifiability between the mean of the source and type effect parameters. 

\paragraph{Specifying Dirichlet priors:}
The simplest Dirichlet priors for the source effects and relative prevalences are symmetric (meaning all of the elements making up the parameter vector $\bm\alpha$ have the same value $\alpha$, called the concentration parameter). Symmetric Dirichlet distributions used as priors when there is no prior knowledge favouring one component over another. When $\alpha$ is equal to one, the symmetric Dirichlet distribution is uniform over all points in its support. Values of the concentration parameter above one prefer variates that are dense, evenly distributed distributions whilst values of the concentration parameter below 1 prefer sparse distributions. A more informative prior can be specified by using a non-symmetric Dirichlet distribution. The magnitude of the vector of $\bm\alpha$ parameters corresponds to the strength of the prior. The relative values of the $\bm\alpha$ vector corresponds to prior information on the comparative sizes of the parameters.

\paragraph{Specifying the Dirichlet Process base distribution and concentration parameters:} The concentration parameter of the DP is specified by the analyst as a modelling decision. The concentration parameter specifies how strong the prior grouping is. In the limit $\alpha\rightarrow 0$, all types will be assigned to one group, increasing $\alpha$ makes a larger number of groups increasingly likely. The Gamma base distribution $Q_{0}$ induces a prior for the cluster locations. This prior should not be too diffuse because if these locations are too spread out, the penalty in the marginal likelihood for allocating individuals to different clusters will be large, hence the tendency will be to overly favour allocation to a single cluster.  This can been seen by considering the marginal posterior for $\theta_k$
\[
\theta_{k} \sim \textsf{Gamma}\left(\alpha_{\theta}+\sum_{i:S_{i}=k}y_{i}, \beta_{\theta}+
\sum_{i:S_{i}=k}\sum_{j=1}^{m}a_{j}\cdot p_{ij}\right)
\]
The term $\sum_{i:S_{i}=k}\sum_{j=1}^{m}a_{j}\cdot p_{ij}$ is very small due to the Dirichlet priors on $\mathbf{a}$ and $\mathbf{r}_j$, hence even a small rate parameter ($\beta_{\theta}$) can 
dominate, leading to the prior having a stronger effect than may be anticipated. 

